{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1QGxtuwJgVCslq4oGke8STitHFz4IYq4A",
      "authorship_tag": "ABX9TyO4aiUwZbyqUCxnsEkFSQSd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aitslab/CellDeathCensus/blob/main/SupplementalData/SupplementalFile2_CellDeathCensus_datacollection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import re\n",
        "import zipfile\n",
        "\n",
        "# Get today's date for file naming\n",
        "today_date = datetime.datetime.today().strftime('%Y%m%d')"
      ],
      "metadata": {
        "id": "-W9KyuK3B8Ly"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deathbase\n",
        "http://www.deathbase.org/\n",
        "\n",
        "Only the manually curated part of this database is to be extracted."
      ],
      "metadata": {
        "id": "4toQhKEnseUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the webpage with Homo sapiens proteins\n",
        "url = 'http://www.deathbase.org/proteins.php?species=H_sapiens'\n",
        "response = requests.get(url)\n",
        "html_content = response.content\n",
        "\n",
        "# Parse the table content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "table = soup.find('table')\n",
        "\n",
        "rows = table.find_all('tr')\n",
        "data = []\n",
        "\n",
        "for row in rows:\n",
        "    cols = row.find_all(['th', 'td'])\n",
        "    cols = [col.text.strip() for col in cols]\n",
        "\n",
        "    # Extract links if available\n",
        "    links = row.find_all('a')\n",
        "    link_list = [link.get('href') if link else '' for link in links]\n",
        "\n",
        "    # Combine data and links\n",
        "    cols.extend(link_list)\n",
        "    data.append(cols)\n",
        "\n",
        "# Construct the DataFrame\n",
        "headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "headers.extend(['Link_' + str(i) for i in range(1, len(link_list)+1)])\n",
        "db_overview_human = pd.DataFrame(data[1:], columns=headers)\n",
        "db_overview_human['Species'] = 'Homo sapiens'\n",
        "\n",
        "# Display the dataframe length\n",
        "print('Deathbase Overview Homo sapiens DataFrame:')\n",
        "print('Number of entries: ' + str(len(db_overview_human)))\n",
        "print('----------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "\n",
        "# Download the webpage with Drosophila melanogaster proteins\n",
        "url = 'http://www.deathbase.org/proteins.php?species=D_melanogaster'\n",
        "response = requests.get(url)\n",
        "html_content = response.content\n",
        "\n",
        "# Parse the table content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "table = soup.find('table')\n",
        "\n",
        "rows = table.find_all('tr')\n",
        "data = []\n",
        "\n",
        "for row in rows:\n",
        "    cols = row.find_all(['th', 'td'])\n",
        "    cols = [col.text.strip() for col in cols]\n",
        "\n",
        "    # Extract links if available\n",
        "    links = row.find_all('a')\n",
        "    link_list = [link.get('href') if link else '' for link in links]\n",
        "\n",
        "    # Combine data and links\n",
        "    cols.extend(link_list)\n",
        "    data.append(cols)\n",
        "\n",
        "# Construct the DataFrame\n",
        "headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "headers.extend(['Link_' + str(i) for i in range(1, len(link_list)+1)])\n",
        "db_overview_fly = pd.DataFrame(data[1:], columns=headers)\n",
        "db_overview_fly['Species'] = 'Drosophila melanogaster'\n",
        "\n",
        "# Display the dataframe length\n",
        "print('Deathbase Overview Drosophila melanogaster DataFrame:')\n",
        "print('Number of entries: ' + str(len(db_overview_fly)))\n",
        "print('----------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "\n",
        "# Download the webpage with Mus musculus proteins\n",
        "url = 'http://www.deathbase.org/proteins.php?species=M_musculus'\n",
        "response = requests.get(url)\n",
        "html_content = response.content\n",
        "\n",
        "# Parse the table content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "table = soup.find('table')\n",
        "\n",
        "rows = table.find_all('tr')\n",
        "data = []\n",
        "\n",
        "for row in rows:\n",
        "    cols = row.find_all(['th', 'td'])\n",
        "    cols = [col.text.strip() for col in cols]\n",
        "\n",
        "    # Extract links if available\n",
        "    links = row.find_all('a')\n",
        "    link_list = [link.get('href') if link else '' for link in links]\n",
        "\n",
        "    # Combine data and links\n",
        "    cols.extend(link_list)\n",
        "    data.append(cols)\n",
        "\n",
        "# Construct the DataFrame\n",
        "headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "headers.extend(['Link_' + str(i) for i in range(1, len(link_list)+1)])\n",
        "db_overview_mouse = pd.DataFrame(data[1:], columns=headers)\n",
        "db_overview_mouse['Species'] = 'Mus musculus'\n",
        "\n",
        "# Display the dataframe length\n",
        "print('Deathbase Overview Mus musculus DataFrame:')\n",
        "print('Number of entries: ' + str(len(db_overview_mouse)))\n",
        "print('----------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "\n",
        "# Download the webpage with C elegans proteins\n",
        "url = 'http://www.deathbase.org/proteins.php?species=C_elegans'\n",
        "response = requests.get(url)\n",
        "html_content = response.content\n",
        "\n",
        "# Parse the table content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "table = soup.find('table')\n",
        "\n",
        "rows = table.find_all('tr')\n",
        "data = []\n",
        "\n",
        "for row in rows:\n",
        "    cols = row.find_all(['th', 'td'])\n",
        "    cols = [col.text.strip() for col in cols]\n",
        "\n",
        "    # Extract links if available\n",
        "    links = row.find_all('a')\n",
        "    link_list = [link.get('href') if link else '' for link in links]\n",
        "\n",
        "    # Combine data and links\n",
        "    cols.extend(link_list)\n",
        "    data.append(cols)\n",
        "\n",
        "# Construct the DataFrame\n",
        "headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "headers.extend(['Link_' + str(i) for i in range(1, len(link_list)+1)])\n",
        "db_overview_worm = pd.DataFrame(data[1:], columns=headers)\n",
        "db_overview_worm['Species'] = 'Caenorhabditis elegans'\n",
        "\n",
        "# Display the dataframe length\n",
        "print('Deathbase Overview Caenorhabditis elegans DataFrame:')\n",
        "print('Number of entries: ' + str(len(db_overview_worm)))\n",
        "print('----------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "\n",
        "# Download the webpage with D rerio proteins\n",
        "url = 'http://www.deathbase.org/proteins.php?species=D_rerio'\n",
        "response = requests.get(url)\n",
        "html_content = response.content\n",
        "\n",
        "# Parse the table content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "table = soup.find('table')\n",
        "\n",
        "rows = table.find_all('tr')\n",
        "data = []\n",
        "\n",
        "for row in rows:\n",
        "    cols = row.find_all(['th', 'td'])\n",
        "    cols = [col.text.strip() for col in cols]\n",
        "\n",
        "    # Extract links if available\n",
        "    links = row.find_all('a')\n",
        "    link_list = [link.get('href') if link else '' for link in links]\n",
        "\n",
        "    # Combine data and links\n",
        "    cols.extend(link_list)\n",
        "    data.append(cols)\n",
        "\n",
        "# Construct the DataFrame\n",
        "headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "headers.extend(['Link_' + str(i) for i in range(1, len(link_list)+1)])\n",
        "db_overview_zebrafish = pd.DataFrame(data[1:], columns=headers)\n",
        "db_overview_zebrafish['Species'] = 'Danio rerio'\n",
        "\n",
        "# Display the dataframe length\n",
        "print('Deathbase Overview Danio rerio DataFrame:')\n",
        "print('Number of entries: ' + str(len(db_overview_zebrafish)))\n",
        "print('----------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "\n",
        "# Merge the five dataframes and sort by Species and Symbol\n",
        "db_overview_5species = pd.concat([db_overview_human, db_overview_fly, db_overview_mouse, db_overview_worm, db_overview_zebrafish], ignore_index=True)\n",
        "db_overview_5species = db_overview_5species.sort_values(by=['Symbol', 'Species'])\n",
        "\n",
        "#Extract UniProt accession numbers from Link_2 column\n",
        "mask = db_overview_5species['Link_2'].str.contains('uniprot', na=False)\n",
        "db_overview_5species.loc[mask, 'UniProt_AC'] = db_overview_5species.loc[mask, 'Link_2'].str.replace('http://www.uniprot.org/uniprot/','')\n",
        "\n",
        "#Extract Ensembl IDs from Link_1 and Link_2 column\n",
        "pattern = r'protein_report.php\\?id=[A-Za-z]_[A-Za-z]+_'\n",
        "db_overview_5species['Ensembl'] = db_overview_5species['Link_1'].str.replace(pattern,'')\n",
        "\n",
        "mask = db_overview_5species['Link_2'].str.contains('ensembl', na=False)\n",
        "pattern = r'http:\\/\\/www\\.ensembl\\.org\\/[A-Za-z]+\\/Gene\\/Summary\\?g='\n",
        "db_overview_5species.loc[mask, 'Ensembl2'] = db_overview_5species.loc[mask, 'Link_2'].str.replace(pattern,'')\n",
        "\n",
        "mask = db_overview_5species['Ensembl2'].notnull()\n",
        "db_overview_5species.loc[mask, 'Ensembl'] = db_overview_5species.loc[mask, 'Ensembl'] + ';' + db_overview_5species.loc[mask, 'Ensembl2']\n",
        "db_overview_5species.drop(columns=['Ensembl2'], inplace=True)\n",
        "\n",
        "# Display the dataframe length\n",
        "print('Deathbase Overview 5 Species DataFrame:')\n",
        "print('Number of entries: ' + str(len(db_overview_5species)))\n",
        "print('----------------------------------------------------------------------')\n",
        "\n",
        "# Save to tsv file on Google drive\n",
        "file_path = f'/content/drive/MyDrive/Deathbase/deathbase_overview_5species_{today_date}.tsv'  # Specify your desired file path\n",
        "db_overview_5species.to_csv(file_path, sep='\\t', index=False)\n",
        "\n",
        "\n",
        "\n",
        "# Download the webpage with full view\n",
        "url = 'http://www.deathbase.org/edit_view.php'\n",
        "response = requests.get(url)\n",
        "html_content = response.content\n",
        "\n",
        "# Parse the table content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "table = soup.find('table')\n",
        "\n",
        "rows = table.find_all('tr')\n",
        "data = []\n",
        "\n",
        "for row in rows:\n",
        "    cols = row.find_all(['th', 'td'])\n",
        "    cols = [col.text.strip() for col in cols]\n",
        "\n",
        "    data.append(cols)\n",
        "\n",
        "# Construct and sort the DataFrame\n",
        "db_fullview = pd.DataFrame(data[1:], columns=data[0])\n",
        "db_fullview.rename(columns={'Name': 'Symbol', 'Refs': 'PMID', 'Link_1': 'Link_Ensembl', 'Link_2': 'Link_UniProt'}, inplace=True) # Rename the Name column to Symbol to match the db_overview_merge dataframe\n",
        "species_mapping = {\n",
        "    'Human': 'Homo sapiens',\n",
        "    'Fly': 'Drosophila melanogaster',\n",
        "    'Zebrafish': 'Danio rerio',\n",
        "    'Mouse': 'Mus musculus',\n",
        "    'Worm': 'Caenorhabditis elegans'\n",
        "}\n",
        "\n",
        "db_fullview['Species'] = db_fullview['Species'].replace(species_mapping) # Rename the species to the latin names to match thedb_overview_merge dataframe\n",
        "db_fullview = db_fullview.sort_values(by=['Symbol', 'Species'])\n",
        "\n",
        "# Display the dataframe length\n",
        "print('Deathbase Fullview DataFrame:')\n",
        "print('Number of entries: ' + str(len(db_fullview)))\n",
        "print('----------------------------------------------------------------------')\n",
        "\n",
        "# Save to tsv file on Google drive\n",
        "file_path = f'/content/drive/MyDrive/Deathbase/deathbase_fullview_{today_date}.tsv'  # Specify your desired file path\n",
        "db_fullview.to_csv(file_path, sep='\\t', index=False)\n",
        "\n",
        "\n",
        "\n",
        "# Merge the 5 species overview and fullview dataframes\n",
        "db_merged = pd.merge(db_overview_5species, db_fullview, on=['Symbol','Species', 'Synonyms', 'Description'])\n",
        "\n",
        "# Rename columns\n",
        "db_merged.rename(columns={\n",
        "    'Process': 'Pathway',\n",
        "    'Pathway': 'Process'\n",
        "    }, inplace=True)\n",
        "\n",
        "# Change the separator to ;\n",
        "db_merged['PMID'] = db_merged['PMID'].str.replace(',', ';')\n",
        "\n",
        "# Drop all rows where the value in 'Pathway' is IMMUNITY\n",
        "db_merged.drop(db_merged[db_merged['Pathway'] == 'IMMUNITY'].index, inplace=True)\n",
        "\n",
        "# Print the length of the merged dataframe\n",
        "print(\"Length of the final merged dataframe:\", len(db_merged))\n",
        "\n",
        "# Save to tsv file on Google drive\n",
        "file_path = f'/content/drive/MyDrive/Deathbase/deathbase_{today_date}.tsv'  # Specify your desired file path\n",
        "db_merged.to_csv(file_path, sep='\\t', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3sL4lkhmafR",
        "outputId": "9eb2e442-4ad2-4cb9-f774-718d6fbb5aa4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deathbase Overview Homo sapiens DataFrame:\n",
            "Number of entries: 86\n",
            "----------------------------------------------------------------------\n",
            "Deathbase Overview Drosophila melanogaster DataFrame:\n",
            "Number of entries: 35\n",
            "----------------------------------------------------------------------\n",
            "Deathbase Overview Mus musculus DataFrame:\n",
            "Number of entries: 62\n",
            "----------------------------------------------------------------------\n",
            "Deathbase Overview Caenorhabditis elegans DataFrame:\n",
            "Number of entries: 11\n",
            "----------------------------------------------------------------------\n",
            "Deathbase Overview Danio rerio DataFrame:\n",
            "Number of entries: 34\n",
            "----------------------------------------------------------------------\n",
            "Deathbase Overview 5 Species DataFrame:\n",
            "Number of entries: 228\n",
            "----------------------------------------------------------------------\n",
            "Deathbase Fullview DataFrame:\n",
            "Number of entries: 228\n",
            "----------------------------------------------------------------------\n",
            "Length of the final merged dataframe: 207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ApoCanD\n",
        "https://webs.iiitd.edu.in/raghava/apocand/"
      ],
      "metadata": {
        "id": "QvXQw8g4diW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the webpage\n",
        "url = 'https://webs.iiitd.edu.in/raghava/apocand/browse.php'\n",
        "response = requests.get(url)\n",
        "html_content = response.content\n",
        "\n",
        "# Parse the table content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "table = soup.find('table')\n",
        "\n",
        "rows = table.find_all('tr')\n",
        "data = []\n",
        "\n",
        "for row in rows:\n",
        "    cols = row.find_all(['th', 'td'])\n",
        "    cols = [col.text.strip() for col in cols]\n",
        "\n",
        "    # Extract links if available\n",
        "    links = row.find_all('a')\n",
        "    link_list = [link.get('href') if link else '' for link in links]\n",
        "\n",
        "    # Combine data and links\n",
        "    cols.extend(link_list)\n",
        "    data.append(cols)\n",
        "\n",
        "# Construct the DataFrame\n",
        "headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "headers.extend(['Link_' + str(i) for i in range(1, len(link_list)+1)])\n",
        "apocand = pd.DataFrame(data[1:], columns=headers)\n",
        "apocand.rename(columns={\n",
        "    'Uniprot': 'UniProt_AC',\n",
        "    'Deathbase': 'Ensembl',\n",
        "    'Protein': 'Symbol',\n",
        "    'Structure': 'PDB',\n",
        "    'Link_2': 'Link_UniProt',\n",
        "    'Link_3': 'Link_Deathbase',\n",
        "    'Link_5': 'Link_PDB'\n",
        "    }, inplace=True) # Rename columns\n",
        "apocand.drop(columns=['BioAssays','Link_1', 'Homologues Protein', 'Link_4', 'Link_6'], inplace=True) # remove columns that are not needed\n",
        "apocand.replace('|', ';', inplace=True) # replace separators\n",
        "\n",
        "apocand['Species'] = 'Homo sapiens' # Add Species information\n",
        "apocand['Pathway'] = 'Apoptosis' # Add Apoptosis information\n",
        "\n",
        "# Display the dataframe length\n",
        "print('ApoCanD DataFrame:')\n",
        "print('Number of entries: ' + str(len(apocand)))\n",
        "print('----------------------------------------------------------------------')\n",
        "\n",
        "# Save to tsv file on Google drive\n",
        "file_path = f'/content/drive/MyDrive/ApoCanD/ApoCanD_{today_date}.tsv'  # Specify your desired file path\n",
        "apocand.to_csv(file_path, sep='\\t', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U56Hd1F1a5ib",
        "outputId": "21de73cc-24ea-4480-925a-75dfa82a29b5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ApoCanD DataFrame:\n",
            "Number of entries: 82\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# yApoptosis\n",
        "http://www.ycelldeath.com/yapoptosis/"
      ],
      "metadata": {
        "id": "vdUefWCcWx9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the yApoptosis database\n",
        "url = 'http://www.ycelldeath.com/yapoptosis/download/yApoptosis.txt'\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200: # Check if the request was successful (status code 200)\n",
        "    filename = f'/content/drive/MyDrive/yApoptosis/yApoptosis_{today_date}.txt' # Specify your desired file path\n",
        "\n",
        "    # Write the content of the response to a file\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "else:\n",
        "    print('Failed to download the file.')\n",
        "\n",
        "# Read the saved file into a pandas DataFrame\n",
        "yApoptosis = pd.read_csv(filename, delimiter='\\t', encoding='cp1252')\n",
        "yApoptosis.dropna(how='all', inplace=True) # drop empty rows\n",
        "\n",
        "yApoptosis['Species'] = 'saccharomyces cerevisiae'\n",
        "yApoptosis.rename(columns={\n",
        "    'gene_id': 'id',\n",
        "    'uniprot': 'UniProt_AC',\n",
        "    'gene_alias': 'Synonyms',\n",
        "    'gene_name': 'Symbol',\n",
        "    'pubmed_id': 'PMID',\n",
        "    'description': 'Description',\n",
        "    'process': 'Process',\n",
        "    'go_id': 'GO'\n",
        "    }, inplace=True) # Rename columns\n",
        "\n",
        "yApoptosis['Pathway'] = 'Apoptosis' # Add Apoptosis information\n",
        "\n",
        "# Change the separator to ;\n",
        "yApoptosis['Synonyms'] = yApoptosis['Synonyms'].str.replace('| ', ';')\n",
        "yApoptosis['Process'] = yApoptosis['Process'].str.replace('|', ';')\n",
        "yApoptosis['GO'] = yApoptosis['GO'].str.replace('|', ';')\n",
        "yApoptosis['PMID'] = yApoptosis['PMID'].str.replace('|', ';')\n",
        "\n",
        "\n",
        "\n",
        "# Display the dataframe length\n",
        "print('yApoptosis DataFrame:')\n",
        "print('Number of entries: ' + str(len(yApoptosis)))\n",
        "print('----------------------------------------------------------------------')\n",
        "\n",
        "# Save to tsv file on Google drive\n",
        "file_path = f'/content/drive/MyDrive/yApoptosis/yApoptosis_{today_date}.tsv'  # Specify your desired file path\n",
        "yApoptosis.to_csv(file_path, sep='\\t', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnTh2qCOW1cA",
        "outputId": "9d05bd6b-d7e5-4add-b1a2-8dfaa29f7dc9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yApoptosis DataFrame:\n",
            "Number of entries: 51\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#UniProt\n",
        "https://www.uniprot.org/"
      ],
      "metadata": {
        "id": "Uz4MWC6hrHgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the UniProt keyword list\n",
        "\n",
        "url = 'https://rest.uniprot.org/keywords/stream?fields=id%2Cname%2Ccategory%2Cgene_ontologies%2Cdefinition%2Csynonyms%2Clinks%2Cchildren%2Cparents%2Cstatistics&format=tsv&query=%28*%29'\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    # Define the filename to save the downloaded file\n",
        "    filename = f'/content/drive/MyDrive/UniProt/UniProt_keywordlist{today_date}.txt' # Specify your desired file path\n",
        "\n",
        "    # Write the content of the response to a file\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "else:\n",
        "    print(\"Failed to download the file.\")\n",
        "\n",
        "# Read the saved file into a pandas DataFrame to create a dataframe for filling in Pathway and Process\n",
        "mapping = pd.read_csv(filename, delimiter='\\t')\n",
        "mapping = mapping[['Keyword ID', 'Name']]\n",
        "\n",
        "keywords = ['KW-0053', 'KW-0381', 'KW-0928', 'KW-0959', 'KW-1061', 'KW-1073', 'KW-1081', 'KW-1082', 'KW-1085', 'KW-1119', 'KW-1210']\n",
        "\n",
        "# Filter the DataFrame to retain only the rows matching the desired keywords\n",
        "mapping = mapping[mapping['Keyword ID'].apply(lambda x: any(keyword in x for keyword in keywords))]\n",
        "\n",
        "# Rename columns\n",
        "mapping.rename(columns={\n",
        "    'Keyword ID': 'UniProt_keyword',\n",
        "    'Name': 'Process'\n",
        "    }, inplace=True)\n",
        "\n",
        "# Fill in the Pathway column\n",
        "pathway_dict = {'KW-0053': 'Apoptosis',\n",
        "        'KW-1073': 'Apoptosis',\n",
        "        'KW-1081': 'Apoptosis',\n",
        "        'KW-1082': 'Apoptosis',\n",
        "        'KW-1085': 'Apoptosis',\n",
        "        'KW-1119': 'Apoptosis',\n",
        "        'KW-1210': 'Necrosis'}\n",
        "\n",
        "mapping['Pathway'] = mapping['UniProt_keyword'].map(pathway_dict)\n",
        "\n",
        "# Display the DataFrame\n",
        "mapping.head(11)\n"
      ],
      "metadata": {
        "id": "6R-4wP10rLcE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "3412503a-7994-4ce9-e459-cd46a6ac2df8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     UniProt_keyword                                            Process  \\\n",
              "49           KW-0053                                          Apoptosis   \n",
              "347          KW-0381                            Hypersensitive response   \n",
              "837          KW-0928                Hypersensitive response elicitation   \n",
              "868          KW-0959                                           Myotoxin   \n",
              "969          KW-1061                                Dermonecrotic toxin   \n",
              "981          KW-1073               Activation of host caspases by virus   \n",
              "988          KW-1081  Inhibition of host apoptosis by viral BCL2-lik...   \n",
              "989          KW-1082  Inhibition of host apoptosis by viral FLIP-lik...   \n",
              "992          KW-1085               Inhibition of host caspases by virus   \n",
              "1025         KW-1119         Modulation of host cell apoptosis by virus   \n",
              "1116         KW-1210                                           Necrosis   \n",
              "\n",
              "        Pathway  \n",
              "49    Apoptosis  \n",
              "347         NaN  \n",
              "837         NaN  \n",
              "868         NaN  \n",
              "969         NaN  \n",
              "981   Apoptosis  \n",
              "988   Apoptosis  \n",
              "989   Apoptosis  \n",
              "992   Apoptosis  \n",
              "1025  Apoptosis  \n",
              "1116   Necrosis  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-006aae3c-1b2e-4c84-8b8e-f712062458fc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UniProt_keyword</th>\n",
              "      <th>Process</th>\n",
              "      <th>Pathway</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>KW-0053</td>\n",
              "      <td>Apoptosis</td>\n",
              "      <td>Apoptosis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>KW-0381</td>\n",
              "      <td>Hypersensitive response</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>837</th>\n",
              "      <td>KW-0928</td>\n",
              "      <td>Hypersensitive response elicitation</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>868</th>\n",
              "      <td>KW-0959</td>\n",
              "      <td>Myotoxin</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>969</th>\n",
              "      <td>KW-1061</td>\n",
              "      <td>Dermonecrotic toxin</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>981</th>\n",
              "      <td>KW-1073</td>\n",
              "      <td>Activation of host caspases by virus</td>\n",
              "      <td>Apoptosis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>988</th>\n",
              "      <td>KW-1081</td>\n",
              "      <td>Inhibition of host apoptosis by viral BCL2-lik...</td>\n",
              "      <td>Apoptosis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>989</th>\n",
              "      <td>KW-1082</td>\n",
              "      <td>Inhibition of host apoptosis by viral FLIP-lik...</td>\n",
              "      <td>Apoptosis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>992</th>\n",
              "      <td>KW-1085</td>\n",
              "      <td>Inhibition of host caspases by virus</td>\n",
              "      <td>Apoptosis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1025</th>\n",
              "      <td>KW-1119</td>\n",
              "      <td>Modulation of host cell apoptosis by virus</td>\n",
              "      <td>Apoptosis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1116</th>\n",
              "      <td>KW-1210</td>\n",
              "      <td>Necrosis</td>\n",
              "      <td>Necrosis</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-006aae3c-1b2e-4c84-8b8e-f712062458fc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-006aae3c-1b2e-4c84-8b8e-f712062458fc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-006aae3c-1b2e-4c84-8b8e-f712062458fc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5e6f6aab-4074-4ff6-84f0-95fdf68eb4b0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5e6f6aab-4074-4ff6-84f0-95fdf68eb4b0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5e6f6aab-4074-4ff6-84f0-95fdf68eb4b0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "mapping",
              "summary": "{\n  \"name\": \"mapping\",\n  \"rows\": 11,\n  \"fields\": [\n    {\n      \"column\": \"UniProt_keyword\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"KW-1073\",\n          \"KW-0053\",\n          \"KW-1119\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Process\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"Activation of host caspases by virus\",\n          \"Apoptosis\",\n          \"Modulation of host cell apoptosis by virus\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pathway\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Necrosis\",\n          \"Apoptosis\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell death-related keywords were selected by manual review of the keyword list: KW-0053, KW-0381, KW-0928, KW-0959, KW-1061, KW-1073, KW-1081, KW-1082, KW-1085, KW-1119, KW-1210\n"
      ],
      "metadata": {
        "id": "pf_PRjjgW9yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# keywords for which reviewed entries are to be downloaded from UniProt\n",
        "keywords = ['KW-0053', 'KW-0381', 'KW-0928', 'KW-0959', 'KW-1061', 'KW-1073', 'KW-1081', 'KW-1082', 'KW-1085', 'KW-1119', 'KW-1210']\n",
        "\n",
        "# download the entries for all keywords and add to a single dataframe\n",
        "\n",
        "# Initialize an empty list to store the DataFrames\n",
        "df_list = []\n",
        "\n",
        "for keyword in keywords:\n",
        "\n",
        "    # Download the reviewed UniProt entries linked to a keyword\n",
        "    url = f'https://rest.uniprot.org/uniprotkb/stream?fields=accession%2Cid%2Cprotein_name%2Cgene_names%2Corganism_name%2Cgene_primary%2Corganism_id%2Cec%2Cxref_ensembl%2Cxref_geneid&format=tsv&query=%28{keyword}%29+AND+%28reviewed%3Atrue%29'\n",
        "\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Define the filename to save the downloaded file\n",
        "        filename = f'/content/drive/MyDrive/UniProt/UniProt_{keyword}_{today_date}.tsv' # Specify your desired file path\n",
        "\n",
        "        # Write the content of the response to a file\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "    else:\n",
        "        print(\"Failed to download the file for keyword:\" + keyword)\n",
        "\n",
        "    # Read the saved file into a pandas DataFrame\n",
        "    df = pd.read_csv(filename, delimiter='\\t')\n",
        "    df['UniProt_keyword'] = keyword\n",
        "\n",
        "    # Display the dataframe length\n",
        "    print(keyword + ' DataFrame:')\n",
        "    print('Number of entries: ' + str(len(df)))\n",
        "    print('----------------------------------------------------------------------')\n",
        "\n",
        "    # Append the DataFrame to the list\n",
        "    df_list.append(df)\n",
        "\n",
        "# Concatenate all DataFrames in the list along rows\n",
        "merge = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# Extract subpart information and strip ending white space from the 'Protein names column'\n",
        "pattern = r'\\[[A-Za-z ]+: .+'\n",
        "merge['Subpart'] = merge['Protein names'].str.findall(pattern).apply(lambda x: ';'.join(x) if x else '')\n",
        "merge['Protein names'] = merge['Protein names'].str.replace(pattern, '') # Remove the extracted values from the 'Protein names' column\n",
        "#merge['Protein names'] = merge['Protein names'].str.rstrip()  # Remove trailing whitespace\n",
        "\n",
        "# Change the separators in the 'Protein names' column\n",
        "\n",
        "def process_protein_names(cell_value):\n",
        "\n",
        "    cell_value = cell_value.rstrip()  # Remove trailing whitespace\n",
        "    extracted_values = []\n",
        "\n",
        "    if cell_value[-1] != ')':  # Check if the last character is not a closing bracket\n",
        "        return cell_value  # Transfer the value to the new column\n",
        "    else:\n",
        "        while len(cell_value) > 0:\n",
        "            closing_brackets = 1\n",
        "            opening_brackets = 0\n",
        "            name = ''\n",
        "\n",
        "            for char in cell_value[::-1][1 if cell_value[-1] == ')' else 0:]:  # Exclude the last character which is a closing bracket\n",
        "                name += char\n",
        "                if char == ')':\n",
        "                    closing_brackets += 1\n",
        "                elif char == '(':\n",
        "                    opening_brackets += 1\n",
        "\n",
        "                if closing_brackets == opening_brackets:  # If counts are equal, exit the loop\n",
        "                    break\n",
        "\n",
        "            value = name[::-1].lstrip('(')  # Reverse the collected characters\n",
        "            extracted_values.append(value)\n",
        "            if value:  # Only remove the extracted value if it's not empty\n",
        "                cell_value = cell_value[:-len(name)-1].rstrip()  # Remove the extracted value from the end of the string\n",
        "\n",
        "        return ';'.join([str(x) for x in extracted_values])\n",
        "\n",
        "# Apply the function to the 'Protein names' column and store the result in a new column 'Protein names 2'\n",
        "merge['Protein names 2'] = merge['Protein names'].apply(process_protein_names)\n",
        "\n",
        "# Change the separator in the 'Gene Names' column from blank to ;\n",
        "merge['Gene Names'] = merge['Gene Names'].str.replace(' ', ';')\n",
        "\n",
        "# Concatenate 'Protein names 2' and 'Gene Names' columns into a new column 'Synonyms' with ';' separator\n",
        "merge['Synonyms'] = merge['Protein names 2'] + ';' + merge['Gene Names']\n",
        "\n",
        "# Rename columns\n",
        "merge.rename(columns={\n",
        "    'Entry': 'UniProt_AC',\n",
        "    'Entry Name': 'UniProt_EntryName',\n",
        "    'Organism': 'Species',\n",
        "    'Gene Names (primary)': 'Symbol',\n",
        "    'Organism (ID)': 'NCBI_TaxID',\n",
        "    'EC number': 'EC',\n",
        "    'GeneID': 'NCBI_GeneID'\n",
        "    }, inplace=True)\n",
        "\n",
        "# Fill in Pathway and Process columns by merging with the keyword mapping dataframe\n",
        "merge2 = merge.merge(mapping, on='UniProt_keyword', how='left')\n",
        "\n",
        "# Display the dataframe length of the merged dataframe (with duplicates)\n",
        "print('Merged DataFrame (with duplicates):')\n",
        "print('Number of entries: ' + str(len(merge2)))\n",
        "print('----------------------------------------------------------------------')\n",
        "\n",
        "# Save merge to tsv file on Google drive\n",
        "filepath = f'/content/drive/MyDrive/UniProt/UniProt_mergewithduplicates_{today_date}.tsv'  # Specify your desired file path\n",
        "merge2.to_csv(filepath, sep='\\t', index=False)\n",
        "\n",
        "#drop Keyword, Pathway and Process column, remove duplicates and save to tsv file on Google drive\n",
        "merge2.drop(columns=['UniProt_keyword', 'Pathway', 'Process'], inplace=True)\n",
        "merge2.drop_duplicates(keep='first', inplace=True, ignore_index=True)\n",
        "\n",
        "filepath = f'/content/drive/MyDrive/UniProt/UniProt_{today_date}.tsv'  # Specify your desired file path\n",
        "merge2.to_csv(filepath, sep='\\t', index=False)\n",
        "\n",
        "# Display the dataframe length of the merged dataframe (without duplicates)\n",
        "print('Merged DataFrame (without duplicates):')\n",
        "print('Number of entries: ' + str(len(merge2)))\n",
        "print('----------------------------------------------------------------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2NiTvrFYHIk",
        "outputId": "9ea71f4b-be0e-41f8-e095-6188886887e5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KW-0053 DataFrame:\n",
            "Number of entries: 2980\n",
            "----------------------------------------------------------------------\n",
            "KW-0381 DataFrame:\n",
            "Number of entries: 84\n",
            "----------------------------------------------------------------------\n",
            "KW-0928 DataFrame:\n",
            "Number of entries: 70\n",
            "----------------------------------------------------------------------\n",
            "KW-0959 DataFrame:\n",
            "Number of entries: 163\n",
            "----------------------------------------------------------------------\n",
            "KW-1061 DataFrame:\n",
            "Number of entries: 218\n",
            "----------------------------------------------------------------------\n",
            "KW-1073 DataFrame:\n",
            "Number of entries: 44\n",
            "----------------------------------------------------------------------\n",
            "KW-1081 DataFrame:\n",
            "Number of entries: 37\n",
            "----------------------------------------------------------------------\n",
            "KW-1082 DataFrame:\n",
            "Number of entries: 3\n",
            "----------------------------------------------------------------------\n",
            "KW-1085 DataFrame:\n",
            "Number of entries: 34\n",
            "----------------------------------------------------------------------\n",
            "KW-1119 DataFrame:\n",
            "Number of entries: 357\n",
            "----------------------------------------------------------------------\n",
            "KW-1210 DataFrame:\n",
            "Number of entries: 109\n",
            "----------------------------------------------------------------------\n",
            "Merged DataFrame (with duplicates):\n",
            "Number of entries: 4099\n",
            "----------------------------------------------------------------------\n",
            "Merged DataFrame (without duplicates):\n",
            "Number of entries: 3848\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FerrDB V2\n",
        "http://www.zhounan.org/ferrdb/current/\n",
        "\n",
        "Database files for Driver, Suppressor, Marker, Unclassified categories has to be downloaded manually from http://www.zhounan.org/ferrdb/current/operations/download.html"
      ],
      "metadata": {
        "id": "o8UERRfZTALp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of database file appendices\n",
        "appendices = ['driver', 'suppressor', 'marker', 'unclassified']\n",
        "\n",
        "# Initialize an empty list to store the DataFrames\n",
        "df_list = []\n",
        "\n",
        "# Loop through the database files and append them to the list\n",
        "for appendix in appendices:\n",
        "    filename = '/content/drive/MyDrive/FerrDB_V2/ferroptosis_'+appendix+'.csv'\n",
        "\n",
        "    # Read the file into a pandas DataFrame\n",
        "    df = pd.read_csv(filename, delimiter=',')\n",
        "    df['FerrDB_V2_category'] = appendix\n",
        "\n",
        "    # Append the DataFrame to the list\n",
        "    df_list.append(df)\n",
        "\n",
        "# Concatenate all DataFrames in the list along rows\n",
        "FerrDB = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# Rename columns\n",
        "FerrDB.rename(columns={\n",
        "    'rcd': 'Pathway',\n",
        "    'symbol': 'Symbol',\n",
        "    'hgncid': 'HGNC_ID',\n",
        "    'ensgstable': 'Ensembl',\n",
        "    'evidence':'Evidence',\n",
        "    'testin': 'Species',\n",
        "    'pathway': 'Process',\n",
        "    'confidence': 'Confidence',\n",
        "    'experiments': 'Experiments',\n",
        "    'caution': 'Caution',\n",
        "    'remark': 'Remark',\n",
        "    'latestupdate': 'Latest_update',\n",
        "    'datasource': 'Source',\n",
        "    'uniformgenetype': 'Type',\n",
        "    'uniprotac': 'UniProt_AC',\n",
        "    'pmid': 'PMID'\n",
        "    }, inplace=True)\n",
        "\n",
        "#Print the species listed in FerrDB_V2_Species\n",
        "species = FerrDB['Species'].unique()\n",
        "print('Unique species terms: ')\n",
        "print(species)\n",
        "print()\n",
        "\n",
        "# Replace values in 'FerrDB_V2_Species' column\n",
        "FerrDB['Species'] = FerrDB['Species'].str.replace('Drosophila', 'fly', case=False)\n",
        "\n",
        "# Create new DataFrames for each species\n",
        "species = ['human', 'rat', 'mice', 'porcine', 'fly', '_NA_']\n",
        "dataframes = {sp: pd.DataFrame() for sp in species}\n",
        "\n",
        "# Count occurrences of each species in the 'FerrDB_V2_Species' column\n",
        "species_counts = {sp: (FerrDB['Species'].str.lower().str.contains(sp.lower())).sum() for sp in species}\n",
        "\n",
        "# Print species counts\n",
        "print(\"Species Counts:\")\n",
        "for sp, count in species_counts.items():\n",
        "    print(f\"{sp}: {count}\")\n",
        "print()\n",
        "\n",
        "# Filter and copy rows to respective DataFrames\n",
        "for sp, df_sp in dataframes.items():\n",
        "    df_sp = FerrDB[FerrDB['Species'].str.contains(sp, case=False)].copy()\n",
        "    df_sp['Species'] = sp\n",
        "    dataframes[sp] = df_sp\n",
        "\n",
        "# Concatenate all DataFrames into a new DataFrame called 'merge'\n",
        "merge = pd.concat([dataframes['human'],\n",
        "                  dataframes['rat'],\n",
        "                  dataframes['mice'],\n",
        "                  dataframes['porcine'],\n",
        "                  dataframes['fly'],\n",
        "                  dataframes['_NA_']],\n",
        "                  ignore_index=True)\n",
        "\n",
        "# Replace species names with latin names\n",
        "merge['Species'] = merge['Species'].str.replace('human', 'Homo sapiens')\n",
        "merge['Species'] = merge['Species'].str.replace('rat', 'Rattus norvegicus')\n",
        "merge['Species'] = merge['Species'].str.replace('mice', 'Mus musculus')\n",
        "merge['Species'] = merge['Species'].str.replace('porcine', 'Sus scrofa')\n",
        "merge['Species'] = merge['Species'].str.replace('fly', 'Drosophila melanogaster')\n",
        "merge['Species'] = merge['Species'].str.replace('unknown', '')\n",
        "\n",
        "# Remove _NA_\n",
        "merge.replace('_NA_', '', inplace=True)\n",
        "\n",
        "# Display the dataframe length of the merged dataframe (with duplicates)\n",
        "print('Merged FerrDB_V2 DataFrame (with duplicates):')\n",
        "print('Number of entries: ' + str(len(merge)))\n",
        "print('----------------------------------------------------------------------')\n",
        "\n",
        "# Remove the identifiers in rows which are not from human as the listed identifiers refer to the human gene/protein\n",
        "merge.loc[merge['Species'] != 'Homo sapiens', ['HGNC_ID', 'Ensembl', 'UniProt_AC']] = ''\n",
        "\n",
        "# Save merge to tsv file on Google drive\n",
        "filepath = f'/content/drive/MyDrive/FerrDB_V2/FerrDB_V2_withduplicates_{today_date}.tsv'  # Specify your desired file path\n",
        "merge.to_csv(filepath, sep='\\t', index=False)\n",
        "\n",
        "# Count unique entries\n",
        "no_dupl = merge[['Species', 'Symbol']].copy()\n",
        "no_dupl.drop_duplicates(inplace=True)\n",
        "\n",
        "# Display the dataframe length of the merged dataframe (without duplicates)\n",
        "print('Merged FerrDB_V2 DataFrame (without duplicates):')\n",
        "print('Number of entries: ' + str(len(no_dupl)))\n",
        "print('----------------------------------------------------------------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gFeTbHmAhTO",
        "outputId": "d3ed0033-4c53-42b1-c336-4bb404829752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique species terms: \n",
            "['Human' 'Mice' 'Human, mice' 'Human, rat' 'Rat' '_NA_' 'Human, porcine'\n",
            " 'Mice, fly' 'Drosophila' 'Human, mice, rat' 'Rats']\n",
            "\n",
            "Species Counts:\n",
            "human: 649\n",
            "rat: 45\n",
            "mice: 284\n",
            "porcine: 2\n",
            "fly: 2\n",
            "_NA_: 6\n",
            "\n",
            "Merged FerrDB_V2 DataFrame (with duplicates):\n",
            "Number of entries: 988\n",
            "----------------------------------------------------------------------\n",
            "Merged FerrDB_V2 DataFrame (without duplicates):\n",
            "Number of entries: 724\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Besides the main database there is extended data from two more studies available for download."
      ],
      "metadata": {
        "id": "ZlZkn1V6AwAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the extended data\n",
        "url = 'http://www.zhounan.org/ferrdb/current/extdownload/ext_20230626.xlsx'\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200: # Check if the request was successful (status code 200)\n",
        "    filename = f'/content/drive/MyDrive/FerrDB_V2/FerrDB_V2_extendeddata_{today_date}.xlsx' # Specify your desired file path\n",
        "\n",
        "    # Write the content of the response to a file\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "else:\n",
        "    print('Failed to download the file.')\n",
        "\n",
        "# Read file into dataframe\n",
        "FerrDB_ext = pd.read_excel(filename)\n",
        "\n",
        "# Rename columns\n",
        "FerrDB_ext.rename(columns={\n",
        "    'RCD': 'Pathway',\n",
        "    'Pathway': 'Process',\n",
        "    'Symbol_or_reported_abbr': 'Symbol',\n",
        "    'ENSG_stable': 'Ensembl',\n",
        "    'Exp_organism': 'Species',\n",
        "    'UniProtAC': 'UniProt_AC',\n",
        "    'Data_source': 'Source',\n",
        "    'Gene_type_hgnc_locus_type_or_other': 'Type'\n",
        "    }, inplace=True)\n",
        "\n",
        "# Replace species names with latin names\n",
        "FerrDB_ext['Species'] = FerrDB_ext['Species'].str.replace('Human', 'Homo sapiens')\n",
        "\n",
        "# Display the dataframe length\n",
        "print('FerrDB_V2 Extended Data DataFrame:')\n",
        "print('Number of entries: ' + str(len(FerrDB_ext)))\n",
        "print('----------------------------------------------------------------------')\n",
        "\n",
        "# Save to tsv file on Google drive\n",
        "file_path = f'/content/drive/MyDrive/FerrDB_V2/FerrDB_V2_extendeddata_{today_date}.tsv'  # Specify your desired file path\n",
        "FerrDB_ext.to_csv(file_path, sep='\\t', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRmM0LQuApaY",
        "outputId": "1674eebf-5a49-4644-c948-caab8631855d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FerrDB_V2 Extended Data DataFrame:\n",
            "Number of entries: 30\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ncRDeathDB, version 2\n",
        "http://www.rna-society.org/ncrdeathdb/"
      ],
      "metadata": {
        "id": "ENah_BunPn3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the extended data\n",
        "url = 'http://www.rna-society.org/ncrdeathdb/data/allNcRNACelldeathData.xlsx'\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200: # Check if the request was successful (status code 200)\n",
        "    filename = f'/content/drive/MyDrive/ncRDeathDB_V2/allNcRNACelldeathData_{today_date}.xlsx' # Specify your desired file path\n",
        "\n",
        "    # Write the content of the response to a file\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "else:\n",
        "    print('Failed to download the file.')\n",
        "\n",
        "# Read file into dataframe\n",
        "ncRDeathDB = pd.read_excel(filename)\n",
        "\n",
        "# Clean Synonyms and Full_name_from_nomenclature_authority columns\n",
        "ncRDeathDB['Full_name_from_nomenclature_authority'] = ncRDeathDB['Full_name_from_nomenclature_authority'].apply(lambda x: x if x != '-' else '')\n",
        "ncRDeathDB['Synonyms'] = ncRDeathDB['Synonyms'].apply(lambda x: x if x != '-' else '')\n",
        "ncRDeathDB['Synonyms'] = ncRDeathDB['Synonyms'].str.replace('|', ';')\n",
        "\n",
        "# Merge columns with synonyms for target to 'Target_Synonyms'\n",
        "ncRDeathDB['Target_Synonyms'] = ncRDeathDB['Synonyms'] + ';' + ncRDeathDB['Full_name_from_nomenclature_authority'] + ';' + ncRDeathDB['Synonyms'] + ';' + ncRDeathDB['Other_designations']\n",
        "\n",
        "# Rename columns\n",
        "ncRDeathDB.rename(columns={\n",
        "    'miRNA_symbol': 'Symbol',\n",
        "    'RNA Category': 'Type',\n",
        "    'Gene_Symbol': 'Target_Symbol',\n",
        "    'Organism': 'Species',\n",
        "    'tax_id': 'NCBI_TaxID',\n",
        "    'geneid':'Target_NCBI_GeneID',\n",
        "    'Description.1': 'Description',\n",
        "    'type_of_gene': 'Target_Type'\n",
        "    }, inplace=True)\n",
        "\n",
        "# Display the dataframe length\n",
        "print('ncRDeathDB_V2 (with duplicates, with autophagy) DataFrame:')\n",
        "print('Number of entries: ' + str(len(ncRDeathDB)))\n",
        "print('----------------------------------------------------------------------')\n",
        "\n",
        "# Save to tsv file on Google drive\n",
        "file_path = f'/content/drive/MyDrive/ncRDeathDB_V2/ncRDeathDB_V2_withduplicates_withautophagy_{today_date}.tsv'  # Specify your desired file path\n",
        "ncRDeathDB.to_csv(file_path, sep='\\t', index=False)\n",
        "\n",
        "# Drop all rows where the value in 'Pathway' is autophagy\n",
        "ncRDeathDB.drop(ncRDeathDB[ncRDeathDB['Pathway'] == 'autophagy'].index, inplace=True)\n",
        "\n",
        "# Save to tsv file on Google drive\n",
        "file_path = f'/content/drive/MyDrive/ncRDeathDB_V2/ncRDeathDB_V2_withduplicates_{today_date}.tsv'  # Specify your desired file path\n",
        "ncRDeathDB.to_csv(file_path, sep='\\t', index=False)\n",
        "\n",
        "# Count unique non-coding RNA entries\n",
        "RNA_no_dupl = ncRDeathDB[['Symbol', 'NCBI_TaxID']].copy()\n",
        "RNA_no_dupl.drop_duplicates(inplace=True)\n",
        "\n",
        "# Display the dataframe length of the non-coding RNA entries (without duplicates)\n",
        "print('ncRDeathDB_V2 non-coding RNAs (without duplicates, without autophagy) DataFrame:')\n",
        "print('Number of entries: ' + str(len(RNA_no_dupl)))\n",
        "print('----------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNRG5KboPuIo",
        "outputId": "75759c0a-81c0-4065-f8f1-7e1468bfef5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-7467cca97cb3>:22: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  ncRDeathDB['Synonyms'] = ncRDeathDB['Synonyms'].str.replace('|', ';')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ncRDeathDB_V2 (with duplicates, with autophagy) DataFrame:\n",
            "Number of entries: 4615\n",
            "----------------------------------------------------------------------\n",
            "ncRDeathDB_V2 non-coding RNAs (without duplicates, without autophagy) DataFrame:\n",
            "Number of entries: 614\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Target information from ncRDeathDB V2\n",
        "path = '/content/drive/MyDrive/ncRDeathDB_V2/ncRDeathDB_V2_withduplicates_20240328.tsv'\n",
        "df_ncrd = pd.read_csv(path, sep='\\t')\n",
        "df_ncrd['Database'] = 'ncRDeathDB V2 (Targets)'\n",
        "\n",
        "# Drop the miRNA-related columns\n",
        "columns = ['id', 'Symbol', 'miRBase_mature_ID', 'miRBase_ID', 'Type', 'Synonyms']\n",
        "df_ncrd.drop(columns=columns, inplace=True)\n",
        "print('ncRDeathDB V2 (Targets)')\n",
        "\n",
        "# Rename columns\n",
        "df_ncrd.rename(columns={\n",
        "    'Target_Symbol': 'Symbol',\n",
        "    'Target_NCBI_GeneID': 'NCBI_GeneID',\n",
        "    'Target_Type': 'Type',\n",
        "    'Target_Synonyms': 'Synonyms'\n",
        "    }, inplace=True)\n",
        "\n",
        "# Drop rows without a target\n",
        "df_ncrd.dropna(subset=['Symbol', 'NCBI_GeneID', 'Synonyms'], how='all', inplace=True)\n",
        "\n",
        "# Display the dataframe length\n",
        "print('ncRDeathDB_V2 Targets (with duplicates) DataFrame:')\n",
        "print('Number of entries: ' + str(len(df_ncrd)))\n",
        "print('----------------------------------------------------------------------')\n",
        "\n",
        "# Save to tsv file on Google drive\n",
        "file_path = f'/content/drive/MyDrive/ncRDeathDB_V2/ncRDeathDB_V2_Targets_withduplicates_{today_date}.tsv'  # Specify your desired file path\n",
        "df_ncrd.to_csv(file_path, sep='\\t', index=False)\n",
        "\n",
        "# Count unique target gene entries\n",
        "no_dupl = df_ncrd[['Symbol', 'NCBI_TaxID']].copy()\n",
        "no_dupl.drop_duplicates(inplace=True)\n",
        "\n",
        "# Display the dataframe length of the target entries (without duplicates)\n",
        "print('ncRDeathDB_V2 target genes (without duplicates, without autophagy) DataFrame:')\n",
        "print('Number of entries: ' + str(len(no_dupl)))\n",
        "print('----------------------------------------------------------------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5Qs2TsVr4Ex",
        "outputId": "70edb5cd-0dae-40dc-ab25-1c47b9a09b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ncRDeathDB V2 (Targets)\n",
            "ncRDeathDB_V2 Targets (with duplicates) DataFrame:\n",
            "Number of entries: 2043\n",
            "----------------------------------------------------------------------\n",
            "ncRDeathDB_V2 target genes (without duplicates, without autophagy) DataFrame:\n",
            "Number of entries: 746\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mitotic Catastrophe Database (MCDB)\n",
        "http://www.combio-lezhang.online/MCDB/\n"
      ],
      "metadata": {
        "id": "YbuKY9Xu43Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the gene and protein data\n",
        "url = 'http://www.combio-lezhang.online/MCDB/gdownload'\n",
        "\n",
        "response = requests.get(url)\n",
        "print(response.status_code)\n",
        "\n",
        "if response.status_code == 200: # Check if the request was successful (status code 200)\n",
        "    filename = f'/content/drive/MyDrive/MCDB/MC-related_gene_and_protein_{today_date}.csv' # Specify your desired file path\n",
        "\n",
        "    # Write the content of the response to a file\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "else:\n",
        "    print('Failed to download the gene and protein file.')\n",
        "\n",
        "# Download the compound data\n",
        "url = 'http://www.combio-lezhang.online/MCDB/cdownload'\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200: # Check if the request was successful (status code 200)\n",
        "    filename_c = f'/content/drive/MyDrive/MCDB/MC-related_compound_{today_date}.csv' # Specify your desired file path\n",
        "\n",
        "    # Write the content of the response to a file\n",
        "    with open(filename_c, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "else:\n",
        "    print('Failed to download the compound file.')\n",
        "\n",
        "\n",
        "# read the gene and protein file into dataframe\n",
        "MCDB = pd.read_csv(filename, sep=',')\n",
        "MCDB.fillna('', inplace=True)\n",
        "\n",
        "# Rename columns\n",
        "MCDB.rename(columns={\n",
        "    'UniProt_Accession': 'UniProt_AC',\n",
        "    'Gene_Name': 'Symbol',\n",
        "    'MC_PMID': 'PMID',\n",
        "    'Synonyms': 'Synonyms_1',\n",
        "    'GO_Identifier': 'GO'\n",
        "    }, inplace=True)\n",
        "\n",
        "# Fill in Pathway column\n",
        "MCDB['Pathway'] = 'Mitotic Catastrophe'\n",
        "\n",
        "# Change separator to ;\n",
        "MCDB['Synonyms_1'] = MCDB['Synonyms_1'].str.replace('|', ';')\n",
        "MCDB['GO'] = MCDB['GO'].str.replace('|', ';')\n",
        "MCDB['PMID'] = MCDB['PMID'].str.replace('|', ';')\n",
        "\n",
        "# Merge columns Protein_Name and Synonyms_1\n",
        "MCDB['Synonyms'] = MCDB['Protein_Name'] + ';' + MCDB['Synonyms_1']\n",
        "\n",
        "# Display the dataframe length\n",
        "print('MCDB (with duplicates) DataFrame:')\n",
        "print('Number of entries: ' + str(len(MCDB)))\n",
        "print('----------------------------------------------------------------------')\n",
        "\n",
        "# Save to tsv file on Google drive\n",
        "file_path = f'/content/drive/MyDrive/MCDB/MCDB_withduplicates_{today_date}.tsv'  # Specify your desired file path\n",
        "MCDB.to_csv(file_path, sep='\\t', index=False)\n",
        "\n",
        "# Count unique entries\n",
        "no_dupl = MCDB[['UniProt_AC']].copy()\n",
        "no_dupl.drop_duplicates(inplace=True)\n",
        "\n",
        "# Display the dataframe length of MCDB (without duplicates)\n",
        "print('MCDB (without duplicates) DataFrame:')\n",
        "print('Number of entries: ' + str(len(no_dupl)))\n",
        "print('----------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k72Eu6r18Xxy",
        "outputId": "e7d94d7b-7fb7-4d1f-fb05-42105000be16"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n",
            "MCDB (with duplicates) DataFrame:\n",
            "Number of entries: 1220\n",
            "----------------------------------------------------------------------\n",
            "MCDB (without duplicates) DataFrame:\n",
            "Number of entries: 1214\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integrated annotations for Programmed Cell Death (iPCD)\n",
        "http://ipcd.biocuckoo.cn/\n",
        "\n"
      ],
      "metadata": {
        "id": "Uv7rbjdhJ6bS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Reviewed part of the database\n",
        "url = 'http://ipcd.biocuckoo.cn/Download/Reviewed.zip'\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200: # Check if the request was successful (status code 200)\n",
        "    filename = f'/content/drive/MyDrive/iPCD/iPCD_Reviewed_{today_date}.zip' # Specify your desired file path\n",
        "\n",
        "    # Write the content of the response to a file\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "else:\n",
        "    print('Failed to download the gene and protein file.')\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "    zip_ref.extract('Reviewed.txt', '/content/drive/MyDrive/iPCD/') # Specify your desired path\n",
        "\n",
        "# Load the data into a DataFrame\n",
        "ipcd = pd.read_csv('/content/drive/MyDrive/iPCD/Reviewed.txt', sep='\\t')\n",
        "ipcd.fillna('', inplace=True)\n",
        "\n",
        "# Split the 'PCD' column into 'Pathway' and 'Effect' columns at the comma\n",
        "ipcd[['Pathway', 'Effect']] = ipcd['PCD'].str.split(',', n=1, expand=True)\n",
        "\n",
        "# Rename columns\n",
        "ipcd.rename(columns={\n",
        "    'UniProt ID': 'UniProt_AC',\n",
        "    'Ensembl Gene ID': 'Ensembl',\n",
        "    'Gene Name': 'Symbol'\n",
        "    }, inplace=True)\n",
        "\n",
        "# Drop all rows where the value in 'Pathway' is Autophagy\n",
        "ipcd.drop(ipcd[ipcd['Pathway'] == 'Autophagy'].index, inplace=True)\n",
        "\n",
        "# Display the dataframe length\n",
        "print('iPCD (with duplicates) DataFrame:')\n",
        "print('Number of entries: ' + str(len(ipcd)))\n",
        "print('----------------------------------------------------------------------')\n",
        "\n",
        "# Count unique entries\n",
        "no_dupl = ipcd[['UniProt_AC']].copy()\n",
        "no_dupl.drop_duplicates(inplace=True)\n",
        "\n",
        "# Display the dataframe length of MCDB (without duplicates)\n",
        "print('iPCD (without duplicates) DataFrame:')\n",
        "print('Number of entries: ' + str(len(no_dupl)))\n",
        "print('----------------------------------------------------------------------')\n",
        "\n",
        "# Save to tsv file on Google drive\n",
        "file_path = f'/content/drive/MyDrive/iPCD/iPCD_{today_date}.tsv'  # Specify your desired file path\n",
        "ipcd.to_csv(file_path, sep='\\t', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTSGyoSBKTpR",
        "outputId": "ae866e54-24b1-483c-97e8-fb1541e52c39"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iPCD (with duplicates) DataFrame:\n",
            "Number of entries: 4399\n",
            "----------------------------------------------------------------------\n",
            "iPCD (without duplicates) DataFrame:\n",
            "Number of entries: 4399\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GO database\n",
        "https://geneontology.org/\n"
      ],
      "metadata": {
        "id": "uUlr5PtUTKO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of cell death-related GO terms\n",
        "id_list = ['0008219',\n",
        "           '0019835',\n",
        "           '0001906',\n",
        "           '0097213',\n",
        "           '0046902',\n",
        "           '0036337',\n",
        "           '0043293',\n",
        "           '0097136',\n",
        "           '1990346',\n",
        "           '0005757']\n",
        "\n",
        "# download the entries for all GO terms and add to a single dataframe\n",
        "\n",
        "# Initialize an empty list to store the DataFrames\n",
        "df_list = []\n",
        "\n",
        "# Define the url to retrieve GO term genes list\n",
        "for id in id_list:\n",
        "    url = f'https://golr-aux.geneontology.io/solr/select?defType=edismax&qt=standard&indent=on&wt=csv&rows=100000&start=0&fl=id,annotation_class_list,annotation_class_list_label,bioentity_label,bioentity_name,synonym,taxon,taxon_label,type,source&facet=true&facet.mincount=1&facet.sort=count&json.nl=arrarr&facet.limit=25&hl=true&hl.simple.pre=%3Cem%20class=%22hilite%22%3E&hl.snippets=1000&csv.encapsulator=&csv.separator=%09&csv.header=false&csv.mv.separator=%7C&fq=document_category:%22bioentity%22&facet.field=source&facet.field=taxon_subset_closure_label&facet.field=type&facet.field=panther_family_label&facet.field=annotation_class_list_label&facet.field=isa_partof_closure_label&q=GO:{id}%0A%0A&qf=bioentity%5E2&qf=bioentity_label_searchable%5E2&qf=bioentity_name_searchable%5E1&qf=bioentity_internal_id%5E1&qf=synonym_searchable%5E1&qf=isa_partof_closure%5E1&qf=isa_partof_closure_label_searchable%5E1&qf=panther_family_searchable%5E1&qf=panther_family_label_searchable%5E1&qf=taxon_label_searchable%5E1'\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200: # Check if the request was successful (status code 200)\n",
        "        filename = f'/content/drive/MyDrive/GO/GO_{id}_{today_date}.tsv' # Specify your desired file path\n",
        "\n",
        "        # Write the content of the response to a file\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "    else:\n",
        "        print('Failed to download the GO file.')\n",
        "\n",
        "\n",
        "    # Read the saved file into a pandas DataFrame\n",
        "    columns = ['id','GO','GOLabels','Symbol','Name','Synonyms_1','NCBI_TaxID','Species','Type','Source']\n",
        "    df = pd.read_csv(filename, sep='\\t', header=None, names=columns)\n",
        "    df['GOSearch'] = 'GO:'+id\n",
        "\n",
        "    # Display the dataframe length\n",
        "    print(id + ' DataFrame:')\n",
        "    print('Number of entries: ' + str(len(df)))\n",
        "    print('----------------------------------------------------------------------')\n",
        "\n",
        "    # Append the DataFrame to the list\n",
        "    df_list.append(df)\n",
        "\n",
        "# Concatenate all DataFrames in the list along rows\n",
        "merge = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# Clean up the columns\n",
        "merge['NCBI_TaxID'] = merge['NCBI_TaxID'].str.replace('NCBITaxon:', '')\n",
        "merge = merge.apply(lambda x: x.str.replace('|', ';'))\n",
        "\n",
        "# Merge columns Name and Synonyms_1\n",
        "merge['Synonyms'] = merge['Name'] + ';' + merge['Synonyms_1']\n",
        "\n",
        "# Extract UniProt ACs\n",
        "merge['UniProt_AC'] = ''\n",
        "merge.loc[merge['id'].str.startswith('UniProtKB:'), 'UniProt_AC'] = merge['id'].str.split(':', expand=True)[1]\n",
        "\n",
        "# Fill in Pathway and Process columns by merging with the mapping dataframe\n",
        "filename =  '/content/drive/MyDrive/GO/GOSearch_mapping_20240404.csv'  # Specify your desired file path with a table containing GOSearch terms and corresponding Process and Pathway terms (from manuscript)\n",
        "mapping = pd.read_csv(filename, sep=',', encoding = 'utf-8') # Read the saved file into a pandas DataFrame\n",
        "merge2 = merge.merge(mapping, on='GOSearch', how='left')\n",
        "\n",
        "# Save to tsv file on Google drive\n",
        "file_path = f'/content/drive/MyDrive/GO/GO_withduplicates_{today_date}.tsv'  # Specify your desired file path\n",
        "merge2.to_csv(file_path, sep='\\t', index=False)\n",
        "\n",
        "# Display the dataframe length of GO (with duplicates)\n",
        "print('GO (with duplicates) DataFrame:')\n",
        "print('Number of entries: ' + str(len(merge)))\n",
        "print('----------------------------------------------------------------------')\n",
        "\n",
        "# Count unique entries\n",
        "no_dupl = merge2[['Symbol','NCBI_TaxID']].copy()\n",
        "no_dupl.drop_duplicates(inplace=True)\n",
        "\n",
        "# Display the dataframe length of GO (without duplicates)\n",
        "print('GO (without duplicates) DataFrame:')\n",
        "print('Number of entries: ' + str(len(no_dupl)))\n",
        "print('----------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOJWgN3DTHvu",
        "outputId": "4d6fbf8e-da1a-411b-b0ae-e5a3134f8b1b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0008219 DataFrame:\n",
            "Number of entries: 9969\n",
            "----------------------------------------------------------------------\n",
            "0019835 DataFrame:\n",
            "Number of entries: 510\n",
            "----------------------------------------------------------------------\n",
            "0001906 DataFrame:\n",
            "Number of entries: 1449\n",
            "----------------------------------------------------------------------\n",
            "0097213 DataFrame:\n",
            "Number of entries: 394\n",
            "----------------------------------------------------------------------\n",
            "0046902 DataFrame:\n",
            "Number of entries: 861\n",
            "----------------------------------------------------------------------\n",
            "0036337 DataFrame:\n",
            "Number of entries: 405\n",
            "----------------------------------------------------------------------\n",
            "0043293 DataFrame:\n",
            "Number of entries: 407\n",
            "----------------------------------------------------------------------\n",
            "0097136 DataFrame:\n",
            "Number of entries: 446\n",
            "----------------------------------------------------------------------\n",
            "1990346 DataFrame:\n",
            "Number of entries: 383\n",
            "----------------------------------------------------------------------\n",
            "0005757 DataFrame:\n",
            "Number of entries: 439\n",
            "----------------------------------------------------------------------\n",
            "GO (with duplicates) DataFrame:\n",
            "Number of entries: 15263\n",
            "----------------------------------------------------------------------\n",
            "GO (without duplicates) DataFrame:\n",
            "Number of entries: 10870\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(mapping.columns)\n",
        "print(merge.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXK19zMCqcjo",
        "outputId": "12725fea-d664-4a43-b25f-ad496b418f32"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['GOSearch,Process,Pathway'], dtype='object')\n",
            "Index(['id', 'GO', 'GOLabels', 'Symbol', 'Name', 'Synonyms_1', 'NCBI_TaxID',\n",
            "       'Species', 'Type', 'Source', 'GOSearch', 'Synonyms', 'UniProt_AC'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XDeath DB\n",
        "https://pcm2019.shinyapps.io/XDeathDB/\n",
        "\n",
        "The csv file with the Cell Death Engine content has to be downloaded manually after selecting Cell death modes \"All\" and Disease type \"All\"."
      ],
      "metadata": {
        "id": "5zJwgYEcCK3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename =  f'/content/drive/MyDrive/XDeathDB/XDeathDB_download_20240404.csv'  # Specify your desired file path\n",
        "\n",
        "# Read the saved file into a pandas DataFrame\n",
        "xdeath = pd.read_csv(filename, sep=',', encoding='latin1')\n",
        "\n",
        "# Add Species column\n",
        "xdeath['Species'] = 'homo sapiens'\n",
        "\n",
        "# Rename columns\n",
        "xdeath.rename(columns={\n",
        "    'symbol': 'Symbol',\n",
        "    'cell_death': 'Pathway'\n",
        "    }, inplace=True)\n",
        "\n",
        "# Drop all rows where the value in 'Pathway' is Autophagy, Proliferation or has no value\n",
        "xdeath.drop(xdeath[xdeath['Pathway'] == 'Autophagy'].index, inplace=True)\n",
        "xdeath.drop(xdeath[xdeath['Pathway'] == 'Proliferation'].index, inplace=True)\n",
        "xdeath.dropna(subset=['Pathway'], inplace=True)\n",
        "\n",
        "# Save to tsv file on Google drive\n",
        "file_path = f'/content/drive/MyDrive/XDeathDB/XDeathDB_withduplicates_withdrugsanddiseases_{today_date}.tsv'  # Specify your desired file path\n",
        "xdeath.to_csv(file_path, sep='\\t', index=False)\n",
        "\n",
        "# Drop disease_name and drugs columns\n",
        "xdeath.drop(['disease_name', 'drugs'], axis=1, inplace=True)\n",
        "xdeath.drop_duplicates(inplace=True)\n",
        "\n",
        "# Save to tsv file on Google drive\n",
        "file_path = f'/content/drive/MyDrive/XDeathDB/XDeathDB_withduplicates_{today_date}.tsv'  # Specify your desired file path\n",
        "xdeath.to_csv(file_path, sep='\\t', index=False)\n",
        "\n",
        "# Display the dataframe length of XDeathDB (with duplicates)\n",
        "print('XDeathDB (with duplicates) DataFrame:')\n",
        "print('Number of entries: ' + str(len(xdeath)))\n",
        "print('----------------------------------------------------------------------')\n",
        "\n",
        "# Count unique entries\n",
        "no_dupl = xdeath['Symbol'].copy()\n",
        "no_dupl.drop_duplicates(inplace=True)\n",
        "\n",
        "# Display the dataframe length of XDeathDB (without duplicates)\n",
        "print('XDeathDB (without duplicates) DataFrame:')\n",
        "print('Number of entries: ' + str(len(no_dupl)))\n",
        "print('----------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "xoSTqF6LUayd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7f17790-0578-49d0-f690-f2a5e5f4259f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-479415e3a83f>:4: DtypeWarning: Columns (0,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  xdeath = pd.read_csv(filename, sep=',', encoding='latin1')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XDeathDB (with duplicates) DataFrame:\n",
            "Number of entries: 9412\n",
            "----------------------------------------------------------------------\n",
            "XDeathDB (without duplicates) DataFrame:\n",
            "Number of entries: 6779\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LncPCD\n",
        "http://spare4.hospital.studio:9000/lncPCD/"
      ],
      "metadata": {
        "id": "1yh5lUEtWcgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Reviewed part of the database\n",
        "url = 'http://spare4.hospital.studio:9000/lncPCD/downloadTwelve.jsp'\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200: # Check if the request was successful (status code 200)\n",
        "    filename = f'/content/drive/MyDrive/LncPCD/LncPCD_{today_date}.txt' # Specify your desired file path\n",
        "\n",
        "    # Write the content of the response to a file\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "else:\n",
        "    print('Failed to download the file.')\n",
        "\n",
        "# Read the saved file into a pandas DataFrame\n",
        "lnc = pd.read_csv(filename, sep='\\t')\n",
        "\n",
        "# Add Type column\n",
        "lnc['Type'] = 'lncRNA'\n",
        "\n",
        "# Rename columns\n",
        "lnc.rename(columns={\n",
        "    'LncRNA Name': 'Symbol',\n",
        "    'Ensembl ID': 'Ensembl',\n",
        "    'LncRNA Regulatory Mechanism': 'Action_Mode',\n",
        "    'Cell Death Processes': 'Pathway',\n",
        "    'Cell Type': 'Tissue'\n",
        "    }, inplace=True)\n",
        "\n",
        "# Drop all rows where the value in 'Pathway' is autophagy\n",
        "lnc.drop(lnc[lnc['Pathway'] == 'autophagy'].index, inplace=True)\n",
        "\n",
        "# Drop columns\n",
        "lnc.drop(['Disease Name', 'Expression', 'Year', 'Title'], axis=1, inplace=True)\n",
        "\n",
        "# Save to tsv file on Google drive\n",
        "file_path = f'/content/drive/MyDrive/LncPCD/LncPCD_withduplicates_{today_date}.tsv'  # Specify your desired file path\n",
        "lnc.to_csv(file_path, sep='\\t', index=False)\n",
        "\n",
        "# Display the dataframe length of LncPCD (with duplicates)\n",
        "print('LncPCD (with duplicates) DataFrame:')\n",
        "print('Number of entries: ' + str(len(lnc)))\n",
        "print('----------------------------------------------------------------------')\n",
        "\n",
        "# Count unique entries\n",
        "no_dupl = lnc['Symbol'].copy()\n",
        "no_dupl.drop_duplicates(inplace=True)\n",
        "\n",
        "# Display the dataframe length of LncPCD (without duplicates)\n",
        "print('LncPCD (without duplicates) DataFrame:')\n",
        "print('Number of entries: ' + str(len(no_dupl)))\n",
        "print('----------------------------------------------------------------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgq2nIDWWfDc",
        "outputId": "79af4e84-2f28-43e1-bdfe-238f553c19e2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LncPCD (with duplicates) DataFrame:\n",
            "Number of entries: 6133\n",
            "----------------------------------------------------------------------\n",
            "LncPCD (without duplicates) DataFrame:\n",
            "Number of entries: 1176\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regulated Cell Death Map\n",
        "https://acsn-curie.lcsb.uni.lu/minerva/index.xhtml?id=Regulated_Cell_Death\n"
      ],
      "metadata": {
        "id": "dSBiVee6jafO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the saved file into a pandas DataFrame\n",
        "filename = '/content/drive/MyDrive/RCDMap/RegulatedCellDeathMap_Minerva_20240404.txt'\n",
        "rcd = pd.read_csv(filename, sep='\\t')\n",
        "\n",
        "# Filter out the relevant entries and clean the dataframe\n",
        "types = ['Protein', 'Antisense RNA', 'RNA', 'Gene']\n",
        "rcd = rcd[rcd['Type'].isin(types)]\n",
        "rcd = rcd.dropna(axis=1, how='all') # Drop all empty columns\n",
        "rcd.drop(['Id', 'Map name', 'Map id', 'Compartment/Pathway id', 'Complex id', 'References'], axis=1, inplace=True)\n",
        "rcd = rcd.applymap(lambda x: str(x).rstrip(',')) # Remove trailing commas from all columns\n",
        "rcd = rcd.applymap(lambda x: str(x).rstrip('*')) # Remove trailing * (used in Name column to indicate commonly used synonyms)\n",
        "rcd.reset_index(drop=True, inplace=True) # Reset the index\n",
        "\n",
        "# Rename columns\n",
        "rcd.rename(columns={\n",
        "    'Synonyms': 'Synonyms_1',\n",
        "    'Compartment/Pathway name': 'Compartment',\n",
        "    'Entrez Gene': 'NCBI_GeneID',\n",
        "    'HGNC': 'HGNC_ID',\n",
        "    'PubMed': 'PMID',\n",
        "    'Uniprot': 'UniProt_AC'\n",
        "    }, inplace=True)\n",
        "\n",
        "# Add Species column and set it to homo sapiens (assumed to be correct as all entries with UniProt_AC came from this species)\n",
        "rcd['Species'] = 'homo sapiens'\n",
        "\n",
        "# Change the separator to ;\n",
        "rcd['PMID'] = rcd['PMID'].str.replace(',', ';')\n",
        "rcd['Synonyms_1'] = rcd['Synonyms_1'].str.replace(',', ';')\n",
        "rcd['Former symbols'] = rcd['Former symbols'].str.replace(',', ';')\n",
        "\n",
        "# Merge columns with synonyms\n",
        "rcd['Synonyms'] = rcd['Synonyms_1'] + ';' + rcd['Name'] + ';' + rcd['Full name'] + ';' + rcd['Former symbols'] + ';' + rcd['HGNC Symbol']\n",
        "\n",
        "# Extract UniProt_AC from the 'Description' column for rows with na\n",
        "mask = rcd['UniProt_AC'] == 'nan'  # Mask for rows with nan in 'UniProt_AC' column\n",
        "rcd.loc[mask, 'UniProt_AC'] = rcd.loc[mask, 'Description'].str.extract(r'UNIPROT:(\\w+)', expand=False)\n",
        "\n",
        "# Extract HGNC_ID from the 'Description' column for rows with na\n",
        "mask = rcd['HGNC_ID'] == 'nan' # Mask for rows with nan in 'HGNC_ID' column\n",
        "rcd.loc[mask, 'HGNC_ID'] = rcd.loc[mask, 'Description'].str.extract(r'HGNC:(\\w+)\\s', expand=False)\n",
        "\n",
        "# Extract Symbol from the 'Description' column for rows with na\n",
        "mask = rcd['Symbol'] == 'nan' # Mask for rows with nan in 'Symbol' column\n",
        "rcd.loc[mask, 'Symbol'] = rcd.loc[mask, 'Description'].str.extract(r'HUGO:(\\w[\\w-]+)\\s', expand=False)\n",
        "\n",
        "# Extract NCBI_GeneID from the 'Description' column for rows with na\n",
        "mask = rcd['NCBI_GeneID'] == 'nan'  # Mask for rows with nan in 'NCBI_GeneID' column\n",
        "rcd.loc[mask, 'NCBI_GeneID'] = rcd.loc[mask, 'Description'].str.extract(r'ENTREZ:(\\w+)\\s', expand=False)\n",
        "\n",
        "# Print the length of the merged dataframe\n",
        "print(\"Length of the RCD Map dataframe (with duplicates):\", len(rcd))\n",
        "\n",
        "# Save to tsv file on Google drive\n",
        "file_path = f'/content/drive/MyDrive/RCDMap/RCDMap_withduplicates_{today_date}.tsv'  # Specify your desired file path\n",
        "rcd.to_csv(file_path, sep='\\t', index=False)\n",
        "\n",
        "print('----------------------------------------------------------------------')\n",
        "\n",
        "# Count unique entries\n",
        "no_dupl = rcd[['Symbol','Name']].copy()\n",
        "no_dupl.drop_duplicates(inplace=True)\n",
        "\n",
        "# Display the dataframe length of LncPCD (without duplicates)\n",
        "print('Length of the RCD Map dataframe (without duplicates)')\n",
        "print('Number of entries: ' + str(len(no_dupl)))\n",
        "print('----------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzYGlt3Jj0iG",
        "outputId": "63332cce-4ba3-4bd6-84ab-a96c7b47f3c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the RCD Map dataframe (with duplicates): 3207\n",
            "----------------------------------------------------------------------\n",
            "Length of the RCD Map dataframe (without duplicates)\n",
            "Number of entries: 1146\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-ce200109c986>:3: DtypeWarning: Columns (26,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  rcd = pd.read_csv(filename, sep='\\t')\n"
          ]
        }
      ]
    }
  ]
}